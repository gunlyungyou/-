{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 기사크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independency\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 파일 명\n",
    "OUTPUT_FILE_NAME = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/신문기사/output.txt'\n",
    "\n",
    "# 긁어 올 URL\n",
    "URL = 'https://www.mk.co.kr/news/stock/view/2018/03/187616/'\n",
    "\n",
    "# 크롤링 함수\n",
    "def get_text(URL):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='Content'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    open_output_file = open(OUTPUT_FILE_NAME, 'w')\n",
    "    result_text = get_text(URL)\n",
    "    open_output_file.write(result_text)\n",
    "    open_output_file.close()\n",
    "    \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 텍스트 정제 모듈\n",
    "    영어, 특수기호 모두 제거\n",
    "\"\"\"\n",
    " \n",
    "import re\n",
    " \n",
    "# 입,출력 파일명\n",
    "INPUT_FILE_NAME = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/신문기사/raw/output.txt'\n",
    "OUTPUT_FILE_NAME = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/신문기사/cleaning/output_cleand.txt'\n",
    " \n",
    " \n",
    "# 클리닝 함수\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    return cleaned_text\n",
    "    \n",
    " \n",
    "# 메인 함수\n",
    "def main():\n",
    "    read_file = open(INPUT_FILE_NAME, 'r')\n",
    "    write_file = open(OUTPUT_FILE_NAME, 'w')\n",
    "    text = read_file.read()\n",
    "    text = clean_text(text)\n",
    "    write_file.write(text)\n",
    "    read_file.close()\n",
    "    write_file.close()\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python [모듈이름] [키워드] [가져올 페이지 숫자] [결과 파일명]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 동아일보 특정 키워드를 포함하는, 특정 날짜 이전 기사 내용 크롤러(정확도순 검색)\n",
    "    python [모듈 이름] [키워드] [가져올 페이지 숫자] [결과 파일명]\n",
    "    한 페이지에 기사 15개\n",
    "\"\"\"\n",
    " \n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    " \n",
    "TARGET_URL_BEFORE_PAGE_NUM = \"http://news.donga.com/search?p=\"\n",
    "TARGET_URL_BEFORE_KEWORD = '&query='\n",
    "TARGET_URL_REST = '&check_news=1&more=1&sorting=3&search_date=1&v1=&v2=&range=3'\n",
    " \n",
    " \n",
    "# 기사 검색 페이지에서 기사 제목에 링크된 기사 본문 주소 받아오기\n",
    "def get_link_from_news_title(page_num, URL, output_file):\n",
    "    for i in range(page_num):\n",
    "        current_page_num = 1 + i*15\n",
    "        position = URL.index('=')\n",
    "        URL_with_page_num = URL[: position+1] + str(current_page_num) \\\n",
    "                            + URL[position+1 :]\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml',\n",
    "                             from_encoding='utf-8')\n",
    "        for title in soup.find_all('p', 'tit'):\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    " \n",
    " \n",
    "# 기사 본문 내용 긁어오기 (위 함수 내부에서 기사 본문 주소 받아 사용되는 함수)\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    content_of_article = soup.select('div.article_txt')\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        output_file.write(string_item)\n",
    " \n",
    " \n",
    "# 메인함수\n",
    "def main(argv):\n",
    "    if len(argv) != 4:\n",
    "        print(\"python [모듈이름] [키워드] [가져올 페이지 숫자] [결과 파일명]\")\n",
    "        return\n",
    "    keyword = argv[1]\n",
    "    page_num = int(argv[2])\n",
    "    output_file_name = argv[3]\n",
    "    target_URL = TARGET_URL_BEFORE_PAGE_NUM + TARGET_URL_BEFORE_KEWORD \\\n",
    "                 + quote(keyword) + TARGET_URL_REST\n",
    "    output_file = open(output_file_name, 'w')\n",
    "    get_link_from_news_title(page_num, target_URL, output_file)\n",
    "    output_file.close()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 조선일보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independency\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL을 가져온 후 text를 가져오는 함수 정의\n",
    "def get_text(URL, output_file):\n",
    "    \n",
    "    #urllib로 news 기사 page요청받기\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    \n",
    "    #BeautiffulSoup을 통해 url로 부터 source code 가져오기\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8'\n",
    "    \n",
    "    #기사의 본문 내용을 추출\n",
    "    content_of_article = soup.select('#news_body_id > div') #추출한 URL에 기사의 본문이 있는 위치\n",
    "    \n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text = True))\n",
    "        output_file.write(string_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL을 가져오는 함수\n",
    "def crawlling(page, category):\n",
    "    page = int(page)\n",
    "    \n",
    "    for page in range(1, page+1):\n",
    "        #제목이 있는 URL\n",
    "        URL_with_page_num = 'http://news.chosun.com/svc/list_in/list.html?catid='+str(category)+'&pn='+str(page)\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "        soup.encoding = 'utf-8'\n",
    "        \n",
    "        i = 1\n",
    "    \n",
    "        for title in soup.find_all('div', attrs={'class': 'petitionsView_title'}):\n",
    "            title_link = title.select('a')\n",
    "            output_file_name = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/신문기사/raw/{category}_page{page}_{i}.txt'.format(category = category, page=page, i = i)\n",
    "            output_file = open(output_file_name, 'w', -1, 'utf-8')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    "            i += 1\n",
    "            \n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawlling(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independency\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL을 가져오는 함수\n",
    "\n",
    "name = 'LG%20CNS'\n",
    "start_date = '2018.01.01'\n",
    "end_date = '2018.03.31'\n",
    "page = '2'\n",
    "\n",
    "url1 = 'https://search.naver.com/search.naver?where=news&query='+name\n",
    "url2 = '&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds='+start_date+'&de='+end_date\n",
    "url3 = '&docid=&nso=so:r,p:from'+start_date+'to'+end_date+',a:all&mynews=0&cluster_rank=38&start='+page\n",
    "\n",
    "\n",
    "def crawlling(name, start_date, end_date, page):\n",
    "    page = int(page)\n",
    "    \n",
    "    for page in range(1, page+1):\n",
    "        \n",
    "        #제목이 있는 URL\n",
    "        URL_with_page_num = url1 + url2 + url3\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'html.parser')\n",
    "        soup.encoding = 'utf-8'\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        for title in soup.find_all('dt'):\n",
    "            title_link = title.select('a')\n",
    "            output_file_name = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/신문기사/raw/{name}_{start_date}_{end_date}_page{page}_{i}.txt'.format(name = name, start_date = start_date, end_date = end_date, page=page, i = i)\n",
    "            output_file = open(output_file_name, 'w', -1, 'utf-8')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    "            i += 1\n",
    "            \n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL을 가져온 후 text를 가져오는 함수 정의\n",
    "def get_text(URL, output_file):\n",
    "    \n",
    "    #urllib로 news 기사 page요청받기\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    \n",
    "    #BeautiffulSoup을 통해 url로 부터 source code 가져오기\n",
    "    soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "    soup.encoding = 'utf-8'\n",
    "    \n",
    "    #기사의 본문 내용을 추출\n",
    "    content_of_article = soup.select('tr >td > div > p') #추출한 URL에 기사의 본문이 있는 위치\n",
    "    \n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text = True))\n",
    "        output_file.write(string_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매일경제 기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independency\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL 가져오는 함수\n",
    "name = ['%BB%EF%BC%BASDI','LG%B5%F0%BD%BA%C7%C3%B7%B9%C0','%B1%E2%BE%C6%C2%F7', 'LG%C0%FC%C0%DA', '%BB%EF%BC%BA%BB%FD%B8%ED', '%C4%DA%BF%FE%C0%CC',\n",
    "        '%C7%F6%B4%EB%C1%A6%C3%B6','%C7%F6%B4%EB%C1%DF%B0%F8%BE%F7']\n",
    "year = [2016, 2015, 2014, 2013, 2012, 2011, 2010, 2008]\n",
    "#'%BB%EF%BC%BASDI' #삼성SDI\n",
    "#'LG%B5%F0%BD%BA%C7%C3%B7%B9%C0' #LG디스플레이 LG%B5%F0%BD%BA%C7%C3%B7%B9%C0%CC\n",
    "#'%B1%E2%BE%C6%C2%F7' #기아차\n",
    "#'LG%C0%FC%C0%DA' #LG전자\n",
    "#'%BB%EF%BC%BA%BB%FD%B8%ED' #삼성생명\n",
    "#'%C4%DA%BF%FE%C0%CC' #웅진코웨이\n",
    "#'%C7%F6%B4%EB%C1%A6%C3%B6' #현대제철\n",
    "#'%C7%F6%B4%EB%C1%DF%B0%F8%BE%F7' #현대중공업\n",
    "    \n",
    "def crawlling(name, year):\n",
    "    page = 35\n",
    "    start_date = '&y1='+str(year)+'&m1=01&d1=01'\n",
    "    end_date = '&y2='+str(year)+'&m2=12&d2=31'\n",
    "\n",
    "    \n",
    "    for page in range(1, page+1):\n",
    "        page = str(page)\n",
    "        \n",
    "        url1 = 'http://find.mk.co.kr/new/search.php?pageNum='+page\n",
    "        url2 = '&cat=&cat1=&media_eco=&pageSize=10&sub=all&dispFlag=OFF&page=news&s_kwd='+name+'&s_page=news&go_page=&ord=1&ord1=1&ord2=0&s_keyword='+name+'&period=p_direct&s_i_keyword='+name\n",
    "        url3 = '&s_author='+start_date+end_date+'&media%5B1%5D=00001&ord=1&area=tt'\n",
    "\n",
    "        #제목이 있는 URL\n",
    "        URL_with_page_num = url1 + url2 + url3\n",
    "        source_code_from_url = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "        soup.encoding = 'utf-8'\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        for element in soup.find_all('div', 'sub_list'):\n",
    "            date = element.find_all('span', 'art_time')\n",
    "            title_link = element.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            dates = re.findall(\"\\d+\", date[0].text)\n",
    "            dates = dates[:3]\n",
    "            dates = '_'.join(dates)\n",
    "            output_file_name = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/data/신문기사/매일경제/'+str(name)+'/'+str(year)+'/{date}_page{page}_{i}.txt'.format(date  =dates, page=page, i = i)\n",
    "            output_file = open(output_file_name, 'w', -1, 'utf-8')\n",
    "            get_text(article_URL, output_file)\n",
    "            i += 1\n",
    "            \n",
    "        output_file.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL을 가져온 후 text를 가져오는 함수 정의\n",
    "def get_text(URL, output_file):\n",
    "    \n",
    "    #urllib로 news 기사 page요청받기\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    \n",
    "    #BeautiffulSoup을 통해 url로 부터 source code 가져오기\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    \n",
    "    #기사의 본문 내용을 추출\n",
    "    string_item = ''\n",
    "    for item in soup.select('div.art_txt'):\n",
    "        string_item += str(item.find_all(text=True))\n",
    "        string_item = re.sub('[a-zA-Z]', '', string_item)\n",
    "        string_item = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', string_item)\n",
    "        output_file.write(string_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawlling('%B1%E2%BE%C6%C2%F7',2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    crawlling('LG%B5%F0%BD%BA%C7%C3%B7%B9%C0%CC',2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국경제 기사 크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independency\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL 가져오는 함수\n",
    "name = ['lg%EC%A0%84%EC%9E%90',\n",
    "        '%EC%82%BC%EC%84%B1%EC%83%9D%EB%AA%85', '%EC%BD%94%EC%9B%A8%EC%9D%B4',\n",
    "        '%ED%98%84%EB%8C%80%EC%A0%9C%EC%B2%A0','%ED%98%84%EB%8C%80%EC%A4%91%EA%B3%B5%EC%97%85','%EA%B8%B0%EC%95%84%EC%B0%A8']\n",
    "year = [2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008]\n",
    "\n",
    "#'%BB%EF%BC%BASDI' #삼성SDI\n",
    "#'LG%EB%94%94%EC%8A%A4%ED%94%8C%EB%A0%88%EC%9D%B4' #LG디스플레이 \n",
    "#'%EA%B8%B0%EC%95%84%EC%B0%A8' #기아차\n",
    "#'lg%EC%A0%84%EC%9E%90' #LG전자\n",
    "#'%EC%82%BC%EC%84%B1%EC%83%9D%EB%AA%85' #삼성생명\n",
    "#'%EC%BD%94%EC%9B%A8%EC%9D%B4' #웅진코웨이\n",
    "#'%ED%98%84%EB%8C%80%EC%A0%9C%EC%B2%A0' #현대제철\n",
    "#'%ED%98%84%EB%8C%80%EC%A4%91%EA%B3%B5%EC%97%85' #현대중공업\n",
    "\n",
    "\n",
    "\n",
    "def crawlling(name, year):\n",
    "    page = 35\n",
    "    start_date = str(year)+'.01.01'\n",
    "    end_date = str(year)+'.12.31'\n",
    "    \n",
    "    for page in range(1, page+1):\n",
    "        page = str(page)\n",
    "        \n",
    "        url1 = 'https://search.hankyung.com/apps.frm/search.news?query='+name\n",
    "        url2 = '&sort=DATE%2FDESC%2CRANK%2FDESC&period=DATE&area=title&mediaid_clust=HKPAPER&sdate=' + start_date + '&edate=' + end_date\n",
    "        url3 = '&exact=&include=&except=&page=' + page\n",
    "        \n",
    "        #제목이 있는 URL\n",
    "        URL_with_page_num = url1 + url2 + url3\n",
    "        source_code_from_url = urllib.request.urlopen(URL_with_page_num)\n",
    "        soup = BeautifulSoup(source_code_from_url, 'html.parser')\n",
    "        soup.encoding = 'utf-8'\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        for element in soup.find_all('div', 'txt_wrap'):\n",
    "            date = element.find_all('span', 'date_time')\n",
    "            title_link = element.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            dates = re.findall(\"\\d+\", date[0].text)\n",
    "            dates = dates[:3]\n",
    "            dates = '_'.join(dates)\n",
    "            output_file_name = 'C:/Users/rjsfu/Dropbox/과제/텍스트마이닝/data/신문기사/한국경제/'+str(name)+'/'+str(year)+'/{date}_page{page}_{i}.txt'.format(date  =dates, page=page, i = i)\n",
    "            output_file = open(output_file_name, 'w', -1, 'utf-8')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    "            i += 1\n",
    "            \n",
    "        output_file.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL을 가져온 후 text를 가져오는 함수 정의\n",
    "def get_text(URL, output_file):\n",
    "    global source_code_from_url\n",
    "    \n",
    "    #urllib로 news 기사 page요청받기\n",
    "    try:\n",
    "        source_code_from_url = urllib.request.urlopen(URL)\n",
    "    except HTTPError as e:\n",
    "        pass\n",
    "\n",
    "    #BeautiffulSoup을 통해 url로 부터 source code 가져오기\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    \n",
    "    #기사의 본문 내용을 추출\n",
    "    string_item = ''\n",
    "    for item in soup.find_all('div', id = 'articletxt'):\n",
    "        string_item += str(item.find_all(text=True))\n",
    "        string_item = re.sub('[a-zA-Z]', '', string_item)\n",
    "        string_item = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', string_item)\n",
    "        output_file.write(string_item)\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawlling('lg%EC%A0%84%EC%9E%90', 2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
